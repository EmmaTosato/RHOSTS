# **Example Folder**

Contains `.sh` files for running different scripts

# Input Data

- Resting-state fMRI data
- From 100 unrelated subjects from the HCP 900 subjects data release
    
    https://zenodo.org/records/7210076#.Y0qhx-9BxhE
    
- Parcellazione corticale: 100 regioni corticali (Schaefer atlas) + 19 regioni subcorticali = **119 ROI**
- Preprocessing
    - Rimozione motion e segnali confondenti (CSF, WM, GSR)
    - Bandpass filtering (0.01‚Äì0.15 Hz)
    - Time series ROI-averaged e z-scored

# **High Order TS**

## simplicial_multivariate.py

### **1. Parallel Processing with Pool**

- Uses Python‚Äôs multiprocessing.Pool to run function calls in parallel across CPU cores.
- Crea un **pool di processi paralleli**, uno per ogni core

### 2. ts_simplicial

- Tutti i processi condividono questo **oggetto**, costruito una volta sola dal dataset
- Struttura contente edges and triplets per tutti i time point
- Pre computazione del simplicial, indicatori etc ‚Üí `simplicial_complex_mvts` in [utils.py](http://utils.py), e parte con `def __init__()`
- Dopo questa riga, `ts_simplicial` ha gi√† in memoria
    - tutti gli edges e triangoli con i relativi z-score e valori massimi
    - le funzioni pronte per generare i complessi simpliciali a ogni istante t, perch√® la classe le contiene

### 3. launch_code_one_t

Per ogni istante temporale viene lanciata la funzione per costruire il simplicial complex ed estrarre topological features.

```python
list_simplices_positive, list_violation_fully_coherence, hyper_coherence = ts_simplicial.create_simplicial_complex(t)
```

- Lista di **simplici**, **violazioni** e **ipercoerenza**.
    
    Dalla funzione `def create_simplicial_complex` nella classe `simplicial_complex_mvts`
    
    Ottengo:
    
    1. `list_simplices_positive`
        1. Simplici (nodi, archi, triplette) nella filtrazione 
        2. Il loro peso. Ricorda che √® stato flippato, quindi dopo il flip abbiamo:
            1. Valori **negativi** ‚áí simplici **coerenti**
            2. Valori **positivi** ‚áí simplici **decoerenti**
    2. In `list_violation_fully_coherence` ogni elemento √® una tupla
        1. Quali sono i triangoli violanti positivi ‚Üí coerenti
        2. Una flag che indica gli spigoli mancanti quando il triangolo tenta di entrare
    3. `hyper_coherence` = l‚Äôipercoerenza
    

```python
dgms1 = compute_persistence_diagram_cechmate(list_simplices_positive)
```

- **Persistence diagram**
    - Lista di 3 array (H0, H1, H2) con 2 colonne: birth and death
    - Ogni riga in ogni array √® una struttura topologica
    
    - Funzioni in utils.py
        - `compute_persistence_diagram_cechmate`
        - `cm.phat_diagrams`

```python
max_filtration_weight = ts_simplicial.find_max_weight(t)
```

- **Massimo valore** assoluto di cofluttuazione (**z-score**) tra edge e triplet al tempo
    1. Qui NON √® arrotondato per eccesso come in def create_simplicial_complex

```python
dgms1_clean = clean_persistence_diagram_cechmate(
        dgms1, max_filtration_weight)
```

- **Sostituisce i punti death = inf**

```python
hyper_complexity = persim.sliced_wasserstein(dgms1_clean, np.array([]))
```

- Computa l‚Äôindicatore **hyper-complexity**
    1. come la **Sliced Wasserstein distance**
    2. Calcola la distanza tra il diagramma attuale ad un tempo $t$ e il diagramma vuoto
    3. Quanto il sistema al tempo $t$ √® **topologicamente complesso /strutturato**
        1. proiettando i punti su pi√π direzioni
        2. confrontando le proiezioni ordinate (L1-distance ‚Üí Manhattan distance)
        
    
    $$
    \text{iper-complessit√†} = \text{distanza di Wasserstein tra il diagramma \( H_1 \) e quello vuoto}
    $$
    

```python
dgms1_complexity_FC = dgms1_clean[(dgms1_clean[:, 0] < 0) & (dgms1_clean[:, 1] <= 0)]
dgms1_complexity_CT = dgms1_clean[(dgms1_clean[:, 0] < 0) & (dgms1_clean[:, 1] > 0)]
dgms1_complexity_FD = dgms1_clean[( dgms1_clean[:, 0] > 0) & (dgms1_clean[:, 1] > 0)]
```

- **Classificazione dei cicli 1D**
    
    Vogliamo sapere se un ciclo proviene:
    
    - interazioni sincronizzate (**FC**)
    - da interazioni anti-sincronizzate (**FD**)
    - da un mix (**CT**).
    
    1. Fully Coherent (FC) ‚Üí birth < 0 e death < 0
        1. Cicli nati e morti solo da interazioni coerenti
    2. Coherent Transition (CT) ‚Üí birth < 0 e death > 0
        1. Cicli che nascono da interazioni coerenti ma vengono chiusi da decoerenti.
    3. Fully Decoherent (FD) ‚Üí birth > 0 e death > 0
        1. Cicli nati e chiusi solo da interazioni decoerenti.
        

```python
complexity_FC = persim.sliced_wasserstein(dgms1_complexity_FC, np.array([]))
```

- **Calcola la distanza Wasserstein**
    1. Il sottoinsieme del diagramma di persistenza selezionato (FC, CT, FD)
    2. Il diagramma vuoto [], che rappresenta uno spazio senza struttura topologica.
    
    ‚áí Questa distanza quantifica quanto √® importante topologicamente quella famiglia di cicli
    
    ‚áí Quanto ‚Äúcomplesso‚Äù √® il contributo FC, CT o FD
    

```python
flag_violations_list = np.array(list_violation_fully_coherence, dtype="object")[:, 2]
```

- **Estrazione delle violazioni**
    
    Estrae il numero di spigoli mancanti per ciascun triangolo violante
    

```python
avg_edge_violation = np.mean(flag_violations_list)
```

- **Calcolo di quanti lati in media mancano nei triangoli che violano la simplicial closure**
    
    indica quanto "gravi" sono mediamente le violazioni
    

```python
edge_weights = compute_edgeweight(list_violation_fully_coherence, n_ROI)
```

- **Costruzione della proiezione downward** della lista delle violazioni Œîv
    1. Vedi spiegazione [funzione](https://www.notion.so/Code-1bb44ec442e080489074d86963b23fd4?pvs=21) + [output](https://www.notion.so/Code-1bb44ec442e080489074d86963b23fd4?pvs=21) data per node strength
    2. Ottengo la lista di edges cos√¨ definita 
        
        $edge\_weights[(i, j)] = [\text{somma\_pesi}, \text{numero\_occorrenze}]$
        
        1. nodo $i$ 
        2. nodo $j$
        3. Somma dei pesi dei triangoli violanti contenenti $(i, j)$
        4. Numero di triangoli che contengono $(i, j)$

## utils.py

### **class simplicial_complex_mvts**

Viene chiamata la casse quando si costruisce l‚Äôoggetto `ts_simplicial` . Questo per tutti i time points con il Pool process.

### def __init__

- Initialization
- **Z-score Normalization** of each signal **(**`compute_zscore_data()`**)**
- z-score?
    
    > Lo z-score misura quanto un valore si discosta dalla media, in termini di deviazioni standard.
    > 
    
    $$
    z = \frac{x- \mu}{ \sigma}
    $$
    

- Shuffling for Null Model (if enabled)
- Precomputes edges and triplets for reducing memory usage (`compute_edges_triplets()`).

### def compute_edges_triplets

### **EDGES**

1. Calcola quanti possibili **archi** si possono formare dai ROI (nodi)
    1. Binomiale 
2. Calcola gli **indici** di tutte le possibili coppie di regioni cerebrali
    1. $i< j$
    2. ROI = 4 
        
        u = [0, 0, 0, 1, 1, 2]
        
        v = [1, 2, 3, 2, 3, 3]
        
        Coppie ROI = (0,1), (0,2), (0,3), (1,2), (1,3), (2,3)
        
3. Calcola la **co-fluttuazione istantanea** tra time series
    - **Co-fluctuation**
        
        > Misuriamo il modo  in cui due nodi (ad esempio, regioni cerebrali) sono collegati nel tempo
        > 
        - $z_i(t)‚ãÖz_j(t)$
            - **1-Order Co-Fluctuation**
            - Element wise product between 2 time series
        - $z_i(t)‚ãÖz_j(t)‚ãÖz_k(t)$
            - **2-Order Co-Fluctuation**
            - Element wise product between 3 time series
        - etc
            - **k-Order Fluctuation**
            - Element wise product between k time series
        
    
    1. Facendo il **prodotto element wise**
        1. Se entrambi i valori sono alti (o bassi), il prodotto √® positivo e grande ‚Üí indica forte co-fluttuazione.
        2. Se uno √® positivo e l‚Äôaltro negativo, il prodotto √® negativo, indicando sincro inversa.
        3. Se uno o entrambi sono vicini a zero, il prodotto √® vicino a zero, cio√® poca sincronizzazione.
    2. `c_prod` √® una matrice: ogni riga √® la serie temporale di una coppia ROI ottenuta dal prodotto element - wise 
    3. Usa batch per processare tot coppie alla volta, tipo ROI = 119
        1. 0 - 117 
        2. 117 - 233
        3. 233 - 348
        4. etc.
4. Calcolo di **media e deviazione** standard
    1. Media nel tempo di ogni riga ‚Üí quindi per ogni coppia  
    2. Deviazione standard nel tempo di ogni riga
    3. Ogni coppia `(i,j)` √® ora rappresentata da [media, std] ‚Üí variabile`ets_zscore`
5. Calcolo **z-score assoluto** e aggiornamento del massimo
    1. E‚Äô un vettore lungo T (time points)
    2. Per ogni tempo $t$, trovo il massimo z-score assoluto tra tutte le coppie del batch
    3. Tiene conto dei batch precedenti quindi alla fine `self.ets_max` contiene,  per ogni istante temporale $t$, il valore pi√π alto (in termini di z-score) trovato tra tutte le coppie di regioni cerebrali analizzate.
    4. *‚ÄúA tempo t, qual √® stata la coppia di ROI pi√π sincronizzata rispetto al suo comportamento normale?‚Äù*
6. Dizionario per associare indice k a coppia $(i, j)$ 

### TRIPLETS

- Numero totale di triplette
    - Ad esempio qua, con 119 ROI ho 273819 combinazioni possibili
    - Considerando che devo passarle tutte e poi fare la moltiplicazione di time series
- Calcolare il prodotto istantaneo
- Calcolare media e deviazione standard nel tempo
- Calcolare z-score assoluto dinamico
- Trovare, per ogni tempo $t$, il massimo z-score assoluto tra tutte le triplette

### def create_simplicial_complex

> Funzione che crea l'elenco dei semplici (e fornisce anche l'elenco delle violazioni)
> 
1. Crea una lista vuota per i complessi
2. `m_weight` √® lo z-score pi√π alto tra tutti gli edges e triplette (si guarda ad`self.ets_max` e `triplets_max` definite prima, che corrispondo ai massimi z-score per edges e triplets ad ogni istante temporale) *arrotondato per eccesso* ad un valore discreto (.ceil)
3. Tutti i nodi entrano insieme, nello stesso momento, e tutti hanno lo stesso peso
4. Computa il peso degli archi, **z-score istantaneo**
    1. Prende i valori 
        1. al tempo `t_current` (√® una colonna) 
        2. e agli indici $i$ e $j$ (righe) 
        
        ‚Üí quindi ho due valori = i valori della time series z-scorati
        
    2. Fa il prodotto 
    3. Lo normalizza ‚Üí z-score
    4. Corregge il peso in base alla regola della coerenza ‚Üí `correction_for_coherence`
        1. Per gli edge in realt√† √® ridondante
        2. Per triplette √® necessaria 
5. Dopo aver chiamato `fix_violations` funzione ritorna
    - `list_simplices_for_filtration` = i simplici nella filtrazione
        - Simplici (nodi, archi, triplette)
        - Il loro peso
    - `list_violations` = i violating triangles
        - Quali sono i triangoli violanti
        - Una flag che indica gli spigoli mancanti quando il triangolo tenta di entrare
    - `percentage_of_triangles_discarded` = l‚Äôipercoerenza
    
    ‚Üí tutto per un istante t 
    

### def correction_for_coherence

- `coherence_function`
- Valori concordi ‚Üí C‚Äô√® coerenza **‚Üí** Il peso viene reso **positivo**
- Valori discordi‚Üí  C‚Äô√® decoerenza **‚Üí** Il peso viene forzato **negativo**

### def fix_violations

> Funzione che rimuove tutti i triangoli violati per creare un filtraggio corretto
> 
- Si **ordinano** i simplici del complesso secondo i loro pesi
- I pesi dei simplici sono ordinati in modo **decrescente**, dal pi√π grande al pi√π piccolo
    - i simplici ‚Äúforti‚Äù (con peso alto) devono entrare prima ‚Üí interazioni pi√π forti ‚Üí pi√π coerenza
    - i simplici ‚Äúdeboli‚Äù (con peso basso), entrano dopo ‚Üí  interazioni pi√π deboli ‚Üí meno coerenza
- Poi nodi + archi vengo inclusi in `list_simplices_for_filtration` (filtrazione)
    - Il peso diventa negativo perch√© dovr√≤ ottenere una lista in ordine crescente ‚Üí necessario per il persistence diagram
        - Quindi il simplicio con peso pi√π alto diventa il pi√π basso
        - il diagramma si aspetta quest‚Äôordine
    - gli indici dei simplici sono messi in `set_simplices`
- Se √® un triangolo, crea le 3 combinazioni di 2 elementi per quel triangolo
    - Se le 3 combinazioni sono presenti nella filtrazione ‚Üí triangolo √® **topologicamente valido**
        - Viene aggiunto a `list_simplices_for_filtration` e peso invertito
        - E se era coerente (peso ‚â• 0), lo conti tra quelli positivi validi
    - Se non tutti i lati sono presenti ‚Üí **violating triangles.** Ma il triangolo pu√≤ essere
        - coerente  (--- o +++)
        - incorente
        
        ‚Üí entrambi vengono salvati ma useremo solo i coerenti (`violation_triangles`) 
        
    
    <aside>
    üìå
    
    In altre parole: 
    
    - noi abbiamo ordinato i pesi dal pi√π grande al pi√π piccolo
    - quindi, se un triangolo entra prima di un suo lato, vuol dire che il peso del triangolo √® maggiore di quel lato
    - e se Il triangolo ha peso maggiore di almeno uno dei suoi lati, allora √® violazione
    - pu√≤ succedere che manchino 2 lati o anche 3, e nell‚Äôultimo caso vuol dire che il peso del triangolo √® maggiore di tutti i 3 lati (si trovano sotto nell‚Äôordinamento crescente fatto all‚Äôinizio)
    </aside>
    
    <aside>
    üìñ
    
    I simplices violanti possono essere considerati **strutture ipercoerenti**, 
    
    - perch√® il loro peso √® > 0
    - poich√© le loro cofluttuazioni di gruppo sono pi√π forti di quelle delle loro parti.
    - Riflettono stati higher-order non catturabili da analisi pairwise.
    </aside>
    
- Vengono anche indicati numero di spigoli mancanti quando il triangolo tenta di entrare
    - flag = 3 ‚Üí tutti gli spigoli sono gi√† presenti nella filtrazione ‚Üí il triangolo pu√≤ entrare
    - flag = 2 , flag = 1 , flag  = 0 ‚Üí mancano `3 - flag` nella filtrazione ‚Üí violating triangles per `3 - flag` spigoli
    
- Calcolo **hyper-coherence** indicator
    - Definition
        
        $$
        \text{iper-coerenza} = \frac{\# \text{triangoli violanti con peso > 0}}{\# \text{triangoli con peso > 0}}
        $$
        
    - How?
        
        Usa solo i triangoli positivi, quindi quelli coerenti
        
        1. `violation_triangles` sono positivi
        2. `triangles_count` sono positivi
        
    
- La funzione ritorna:
    - `list_simplices_for_filtration` = i simplici nella filtrazione
        - Simplici (nodi, archi, triplette)
        - Il loro peso
    - `list_violating_triangles` = i violating triangles
        - Quali sono i triangoli violanti
        - Una flag che indica gli spigoli mancanti quando il triangolo tenta di entrare
    - `hyper_coherence` = l‚Äôipercoerenza
    
    ‚Üí tutto per un istante t 
    

### def coherence_function

- Regola della coerenza pura
    - Se entrambi positivi‚Üí  segnali sincronizzati in alto ‚Üí concordi
    - Se entrambi negativi ‚Üí segnali sincronizzati in basso ‚Üí concordi
    - Se uno + e uno ****‚Üí ****segnali opposti ‚Üí discordi
- Posso avere exponent = 0 o exponent = 1
- Quindi facendo l‚Äôesponenziale di -1 posso avere
    - res = 1 ‚Üí **fully coherent**
    - res = -1 ‚Üí **decoherent**
- Per gli EDGE
    
    
    | ROI1 | ROI2  | Coerenza? |
    | --- | --- | --- |
    | **+** | **+**  | ‚úÖ s√¨ |
    | **‚Äì** | **‚Äì**  | ‚úÖ s√¨ |
    | **+** | **‚Äì**  | ‚ùå no |
- Per le TRIPLETTE
    
    
    | ROI1 | ROI2 | ROI3 | Coerenza? |
    | --- | --- | --- | --- |
    | **+**  | **+** | **+** | ‚úÖ s√¨ |
    | **-** | **-** | **-** | ‚úÖ s√¨ |
    | **+** | **-** | **+** | ‚ùå no |
    | **-** | **-** | **+** | ‚ùå no |

### def clean_persistence_diagram_cechmate

- con order = 1 seleziona `dgms[1]`
- Quindi per per default, si lavora con $H‚ÇÅ$ = i cicli
- Poi scorre le coppie `(birth, death)` nel diagramma di persistenza di $H‚ÇÅ$.
- Se un feature ha death = ‚àû ‚Üí lo sostituisce con `max_filtration` (cio√® il massimo peso usato a quel tempo t).

‚Üí E‚Äô qui che mi tengo solo il gruppo omologico $H_1$

### def compute_persistence_diagram_cechmate

- `cm` √® il modulo [Cechmate](https://github.com/scikit-tda/cechmate/blob/master/cechmate/solver.py#L63), una libreria per la topological data analysis (**TDA**).
- In un diagramma di persistenza abbiamo una lista di coppie di numeri (**birth, death**).
    
    > ‚ÄúQuali strutture topologiche (componenti, cicli, cavit√†) emergono nei  dati e quanto a lungo persistono nella filtrazione simpliciale.‚Äù
    > 
    - Se birth e death sono molto lontani ‚Üí la struttura √® persistente, quindi importante
    - Se birth ‚âà death ‚Üí √® rumore
    - Se death = ‚àû ‚Üí la struttura non scompare mai
    
    <aside>
    üìñ
    
    La differenza death ‚Äì birth √® la **persistenza** ‚Üí pi√π √® alta, pi√π la struttura √® stabile
    
    </aside>
    
- Noi otteniamo `dgms`, una lista di 3 array, ciascuno contiene un **diagramma di persistenza**.
    
    
    | Indice | Nome | Cosa misura |
    | --- | --- | --- |
    | `dgms[0]` | features di **$H_0$** | Componenti connesse) |
    | `dgms[1]` | features di **$H‚ÇÅ$** | Loop |
    | `dgms[2]` | features di **$H‚ÇÇ$** | Cavit√†  |

- In ognuno di questi array  ha quindi
    - 2 colonne = birth and death
        1. colonna 0 = birth 
            1. valore della filtrazione quando la feature appare
            2. peso del simplesso che ha dato origine al feature
        2. colonna 1 = death 
            1. valore della filtrazione quando la feature scompare
            2. peso del simplesso che lo ha fatto sparire.
    - n_features = n righe che corrispondono alle topological features

### def compute_edgeweight

- Itera su tutte le violazioni (`list_violations`)
    - triangolo $[i, j, k]$
    - weight √® il peso della co-fluttuazione triadica
- Ciclo su tutte le 3 combinazioni di spigoli nel triangolo
    - $(i,j), (i,k), (j,k)$
    - Se lo stesso spigolo $(i, j)$ √® gi√† comparso in un triangolo precedente della lista `list_violations:`
        - somma il nuovo peso a quello gi√† accumulato
        - Incrementa il conteggio di 1
        - Altrimenti, lo aggiungi per la prima volta
- Quindi `edge_weight` √® un dizionario in cui
    - le chiavi $(i, j)$ rappresentano gli archi
    - I valori $[w_{sum}, count]$ rappresentano:
        - la somma dei pesi dei triangoli violanti che includono quell‚Äôarco
        - quanti triangoli violanti contengono quell‚Äôarco
- Concettualmente:
    - **Proietto** perch√© attribuisco il peso del triangolo $w_{ijk}$ a ciascuno dei suoi spigoli $(i,j), (i,k), (j,k)$
    - Se un arco $(i, j)$ appare molto spesso nei triangoli violanti $Œîv$, significa √® costantemente parte di triangoli che mostrano forte co-fluttuazione triadica ‚Üí quindi coinvolto in interazioni sincronizzate di ordine superiore

# High Order TS Scaffold

## utils.py

### def fix_violations

> Funzione che rimuove tutti i triangoli violati per creare un filtraggio corretto
> 
- *Vedi sopra per spiegazione*
- `list_simplices_scaffold_all`
    - contiene tutti i simplici topologicamente validi. Include:
        - nodi
        - archi
        - triangoli coerenti che rispettano la simplicial closure
    - OrderedDict()
        - La chiave √® il simplicio (lista di vertici) convertito in stringa
        - Il valore √® una lista:
            1. Indice di ordine = l‚Äôindice in cui entra nella filtrazione
            2. Peso negativo = segno invertito per costruire la filtrazione crescente
- Ogni nodo viene inserito con peso negativo e indice fisso perch√© entrano tutti "insieme" nel complesso
- Per gli archi
    - Se l‚Äôarco ha peso diverso da quello precedente, incrementa l‚Äôindice
    - Cos√¨ simplici con lo stesso peso hanno lo stesso ordine di ingresso.
    - Ricorda: Essendo che i simplici sono ordinati, i pesi uguali saranno vicini
- Per i triangoli
    - I triangoli che rispettano la closure (i loro 3 lati sono gi√† nel complesso) sono inseriti nello scaffold.
    - Il peso viene invertito, come da convenzione nel persistence diagram.

### compute_scaffold (...)

- Funzione che chiama jython code per computare lo scaffold
- **Homological scaffold**
    - Un grafo pesato costruito dai generatori di H‚ÇÅ
    - Ogni ciclo persistente diventa una struttura concreta fatta di archi (edges) e nodi (nodes).
- Un grafo pesato costruito dai generatori di H‚ÇÅ: ogni ciclo persistente diventa una struttura concreta fatta di archi (edges) e nodi (nodes).
- Serve per:
    - capire quali archi sono topologicamente rilevanti
    - rappresentazione grafo like

‚Üí struttura topologica concreta che ti dice dove √® localizzata la complessit√†

## persistent_homology_calculation.py

> Calcolare la persistent homology (in particolare H‚ÇÅ) a partire da un dizionario di simplici filtrati, e salvare i generatori persistenti come cicli concreti (liste di archi) in un file `.pck`.
> 

- converte la stringa in formato JSON ordinato (`OrderedDict`)
- estrae i parametri da sys.argv: dimensione massima, directory output, tag output, path a javaplex, flag salvataggio
- costruisce una lista dei pesi dei simplici e li ordina
- crea un oggetto `ExplicitSimplexStream`
    - una stream √® una sequenza ordinata di simplici, ciascuno con:
    - Serve come input al calcolo della persistent homology. Infatti Javaplex usa la stream per:
        - sapere quali simplici ci sono
        - e in che ordine entrano nel complesso
- Per ogni simplex nel dizionario:
    - se √® un vertice, lo aggiunge al tempo 0
    - altrimenti lo aggiunge con l‚Äôindice di ingresso fornito
- chiude la stream con `finalizeStream()`
- crea un algoritmo per la persistent homology su campo Z‚ÇÇ fino a dimensione `dimension + 1`
- calcola
    - gli intervalli di persistenza (`computeIntervals`)
    - anche i generatori annotati (`computeAnnotatedIntervals`)
- per ogni dimensione da 1 a dimensione massima:
    - estrae la lista degli intervalli (birth, death) e dei generatori associati
    - per ogni generatore:
        - costruisce i cicli usando `Holes.Cycle` con: dimensione, lista simplici, peso al birth, peso al death
        - aggiunge il ciclo al dizionario dei generatori
- salva l‚Äôintero dizionario dei generatori in un file `.pck` nella cartella `gen/`
- stampa a terminale le informazioni base (filtrazione creata, dimensioni, validit√†, path file generato)

# Output

## Indicators

A row for each time point (the range is defined in the input)

1. Time
2. Hyper complexity indic.
3. Hyper complexity FC
4. Hyper complexity CT 
5. Hyper complexity FD
6. Hyper coherence indicator
7. Average edge violation
8. (Edge weight in formato hdf5 se indicato nelle flag)

![Screenshot 2025-07-01 alle 11.51.09.png](attachment:2a4c931a-e85a-4a64-8390-b3baf432a055:Screenshot_2025-07-01_alle_11.51.09.png)

## Node strength projecting

Proiettando la forza nodale:

1. $\Delta V$ approach: dai triangoli ‚Üí archi ‚Üí nodi.
2. Scaffold approach: dai cicli persistenti ‚Üí archi ‚Üí nodi.

### What is Nodal strength?

In una rete **pesata**, la forza del nodo $i$ √® la **somma dei pesi degli archi incidenti** a $i$. √à l‚Äôanalogo pesato del grado.

$$
‚Åç
$$

- **Non orientata:** $w_{ij}=w_{ji}$ e si sommano tutti gli archi che toccano $i$.
- **Orientata:**
    - **out-strength** $s_i^{\text{out}}=\sum_j w_{ij}$
    - **in-strength** $s_i^{\text{in}}=\sum_j w_{ji}$

In reti **non pesate** (tutti i $w_{ij}\in\{0,1\}$), la forza coincide con il **grado** del nodo.

<aside>
üîë

Non conta *quanti* vicini ha (quello √® il grado), ma quanto forti sono, nel complesso, i legami che lo collegano al resto della rete. 

</aside>

### From the violating triangles ‚àÜv

- Fig. 5 Paper - Mapping
    
    *‚ÄúFigure 5a reports a brain map of the most discriminative nodes by projecting the magnitudes of the violating triangles Œîv on a nodal level. This is equivalent to considering the nodal strength extracted from Œîv.‚Äù*
    
- Methods
    
    *‚ÄúTo analyse the information provided by the list of violations $Œîv$ on an edge/node level, we rely on downwards projections. That is, for each edge $(i, j)$, we assign a weight $w_{ij}$ equal to the average sum of the weights of triangles defined by that edge, that is, triangles of the form $(i, j, ¬∑)$ with a weight $w_{ij¬∑}$, and the average is computed over the number of triangles $n_{ij¬∑}$ defined by that edge.* 
    
    *Similarly, we define the nodal strength $w_i$ of node $i$ as the sum of weights of the triangles connected to node i after the edge projection.‚Äù*
    
- üìñ¬†Downwards projections and Nodal strength
    - Œîv √® la lista dei triangoli che violanti
    - Ogni triangolo ha un peso  ‚Üí misura la forza della violazione.
    - Considera un arco $(i,j)$, questo arco pu√≤ appartenere a pi√π triangoli violanti $(i,j,k)$.
    - il peso dell‚Äôarco $w_{ij}$ = media dei pesi dei triangoli che contengono $(i,j)$
    - Ogni arco ‚Äúeredita‚Äù un peso che riassume l‚Äôinfluenza dei triangoli violanti a cui partecipa.
    - Ora hai un grafo di archi pesati
    - La forza nodale di un nodo $i$, $w_i$, si calcola come la somma dei pesi di tutti gli archi che toccano $i$
    - Cos√¨ un nodo ha pi√π forza se partecipa a molti archi che a loro volta sono coinvolti in triangoli violanti ad alto peso.
    
    <aside>
    üîë
    
    Proiettiamo i triangoli prima sugli archi e poi sui nodi.
    
    </aside>
    
- üíª¬† Computation of Downwards projections - Nodal strength
    - `compute_nodal_strength`
    - Partiamo dal file hdf5 con un dataset per ogni **istante temporale**
    - Ogni dataset ha una shape ($N_\text{triangles}$, 4)
        - E‚Äô l‚Äôoutput di compute_edgeweight in utils.py
        - Quindi ogni riga √® un triangolo violante
        - Le colonne indicano questo:
            1. nodo $i$ 
            2. nodo $j$
            3. Somma dei pesi dei triangoli violanti che includono $(i, j)$ ‚Üí `sum_w`
            4. Numero di triangoli violanti che contengono $(i, j)$ ‚Üí `count`
    - Loop sulle righe del dataset precedente
        - Prendo ogni arco e ne calcolo la media perch√© dal dataset ho la somma e il count dei triangoli
        - Salvo questo peso mediato nel dizionario `edge_weight`
    - Proiezione archi ‚Üí nodi (forza nodale)
        - per ogni riga nel dizionario precedente
        - sommo  `w_ij` sia a `nodal_strength[i]` sia a `nodal_strength[j]` ottenendo cos√¨ la nodal strength per ogni nodo
- Fig. 5 Paper - 15% most coherent
    - txt con risultati
    - Prendo la colonna dell‚Äôiper coerenza
    - Metto in ordine decrescente
    - Prendo il 15% dei time points (= righe) pi√π coerenti
    

### From the Homological Scaffold

- Definition of Homological Scaffold
    
    *‚ÄúTo obtain a finer description of the topological features present in the persistence diagram, we consider the persistence homological scaffold as proposed in ref. 8. In a nutshell, this object is a weighted network composed of all the cycle paths corresponding to generators gi weighted by their persistence $œÄ_{g_i}$. In other words, if an edge e belongs to multiple one-dimensional cycles $g_0, g_1, ..., g_s$, its weight $\bar w^{\pi}e$ is defined as the sum of the generators‚Äô persistence*
    
    $$
    \bar w^{\pi}e \;=\; \sum{g_i:\; e\in g_i} \pi_{g_i}.
    $$
    
- üìñ¬†Homological Scaffold
    - Rete pesata
    - Formata da tutti i cammini ciclici dei generatori di una certa dimensione
        - Ovvero: gli archi del grafo sono l‚Äôunione di tutti gli archi che compaiono nei generatori
        - Esempio: tutti gli archi che compaiono nei loop (generatori 1D) in $~~H_1~~$
    - Il peso di un arco $e$ √® la persistenza $\pi$ del generatore $g$
        - Se l‚Äôarco $e$ compare in due generatori con persistenze $a$ e $b$ allora il peso sar√† la somma di queste persistenze
- üíª¬† Computation of Homological Scaffold
    - Partiamo dal file .pck ad **un** certo **time point**
    - Ci dice quali sono i gruppi di omologia presenti, troveremo sempre H1
    - Dentro ci sono **generatori** omologici H1
        - ovvero i generatori non trivial
        - i loop = cicli 1D persistenti nel tempo
        - ognuno di questi ci dice
            - quali archi formano il loop,
            - la loro persistenza (birth‚Äìdeath interval).
    - Costruzione **scaffold**
        - √® una rete pesata
        - gli **archi** sono l‚Äôunione di tutti gli archi che compaiono nei cicli
        - il peso di un arco √® la **somma** della **la persistenza di tutti i cicli** che contengono quell‚Äôarco
- Fig. 5 Paper - Mapping
    
    *‚ÄúSimilar analyses can be produced by focusing on the hyper-complexity indicator and the nodal strength of the homological scaffold constructed from the persistent homology generators of H1.*
    
- Methods
    
    *‚ÄúIn the case of the homological scaffold, since it is a weighted network, the node strength $\bar{w}_i$ of node  $i$ is defined, in the classical way, as the sum of the weights of edges connected to the node $i$.‚Äù*
    
- üìñ¬†Nodal strength
    - Dalla rete pesata abbiamo gli archi pesati
    - Ora dobbiamo proiettare sui nodi
    - La forza di un nodo $i$ √® definita come la somma degli archi connessi a quel nodo $i$
        - Ad esempio se abbiamo arco $(i, j)$ con $w = 4$ e $(i,k)$ con $w = 6$, la forza di i sar√† $10$
    - Questo misura quanto un nodo √® coinvolto (globalmente) nei cicli persistenti.
    
- üíª¬† Computation of  Nodal strength
    - La forza nodale √® cio√® la somma dei pesi degli archi incidenti al nodo $i$
- Fig. 5 Paper - 15% lowest complexity
    
    *In particular, Fig. 5c depicts the brain map obtained when isolating the 15% of frames with lowest hyper-complexity, which are those associated with more synchronized dynamical phases.‚Äù*
    

## Hyper Complexity

*‚ÄúBy construction, the these three contributions sum up to the total hyper-complexity.‚Äù*

 

- Plottare i punti (uno per ogni timepoint) in uno **spazio ternario** (triangolo equilatero)
- Ogni punto ha coordinate derivate dai **contributi normalizzati**:
    - FC = fully coherent
    - CT = coherence transition
    - FD = fully decoherent
- FC, CT, FD sono componenti non negative che sommano a un totale ‚Üí l‚Äôindicatore di **ipercomplessit√†**
- Si normalizza perch√® la posizione di un punto nel triangolo non dipende dai valori assoluti
- Poi si estraggono le coordinate per plottare il contributo di queste coordinate in un punto